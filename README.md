# Ecommerce Data Analysis

AnÃ¡lisis de datos de ecommerce con pipeline automatizado para cargar datos a Supabase.

## ğŸš€ CaracterÃ­sticas

- **Pipeline de Datos Automatizado**: Carga automÃ¡tica de archivos CSV a Supabase
- **ValidaciÃ³n de Datos**: VerificaciÃ³n de calidad y limpieza automÃ¡tica
- **Interfaz Web**: Dashboard interactivo con Streamlit
- **AnÃ¡lisis de Negocio**: MÃ©tricas y visualizaciones de KPIs

## ğŸ“ Estructura del Proyecto

```
mockup/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ data/                    # Archivos CSV de datos
â”‚   â”‚   â”œâ”€â”€ customer_dim.csv
â”‚   â”‚   â”œâ”€â”€ fact_table.csv
â”‚   â”‚   â”œâ”€â”€ item_dim.csv
â”‚   â”‚   â”œâ”€â”€ store_dim.csv
â”‚   â”‚   â”œâ”€â”€ time_dim.csv
â”‚   â”‚   â””â”€â”€ Trans_dim.csv
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ pipelines/           # Pipeline de datos
â”‚   â”‚   â”‚   â””â”€â”€ data_pipeline.py
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â”œâ”€â”€ readers/         # Lectura de archivos
â”‚   â”‚   â”‚   â”œâ”€â”€ writers/         # Escritura a BD
â”‚   â”‚   â”‚   â””â”€â”€ validators/      # ValidaciÃ³n de datos
â”‚   â”‚   â”œâ”€â”€ utils/               # Utilidades (conexiÃ³n BD)
â”‚   â”‚   â””â”€â”€ app/                 # AplicaciÃ³n Streamlit
â”‚   â”œâ”€â”€ run_pipeline.py          # Script principal del pipeline
â”‚   â”œâ”€â”€ run_pipeline_simple.py   # Script simple del pipeline
â”‚   â””â”€â”€ test_pipeline.py         # Script de pruebas
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ› ï¸ InstalaciÃ³n

1. **Clonar el repositorio**:
   ```bash
   git clone <repository-url>
   cd mockup
   ```

2. **Instalar dependencias**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configurar variables de entorno**:
   - Crear archivo `app/dev.env` con las credenciales de Supabase:
   ```
   USER=tu_usuario
   PASSWORD=tu_password
   HOST=tu_host.supabase.com
   PORT=6543
   DBNAME=postgres
   ```

## ğŸ“Š Uso del Pipeline

### OpciÃ³n 1: Desde la lÃ­nea de comandos

**Ejecutar pipeline completo**:
```bash
cd app
python run_pipeline_simple.py
```

**Ejecutar con logging detallado**:
```bash
cd app
python run_pipeline.py
```

### OpciÃ³n 2: Desde la aplicaciÃ³n web

1. **Iniciar la aplicaciÃ³n**:
   ```bash
   cd app
   streamlit run src/app/main.py
   ```

2. **Usar el sidebar**:
   - Expandir "ğŸ“Š Pipeline de Datos"
   - Hacer clic en "ğŸš€ Ejecutar Pipeline Completo"
   - O seleccionar archivo individual para cargar

### OpciÃ³n 3: Cargar archivos individuales

```python
from src.data.writers.save_db import save_single_file

# Cargar un archivo especÃ­fico
success = save_single_file("path/to/file.csv", "table_name")
```

## ğŸ”§ Funcionalidades del Pipeline

### âœ… ValidaciÃ³n de Datos
- VerificaciÃ³n de archivos vacÃ­os
- DetecciÃ³n de columnas vacÃ­as
- IdentificaciÃ³n de duplicados
- ValidaciÃ³n de tipos de datos

### ğŸ§¹ Limpieza AutomÃ¡tica
- EliminaciÃ³n de filas completamente vacÃ­as
- Limpieza de espacios en blanco
- ConversiÃ³n automÃ¡tica de fechas
- OptimizaciÃ³n de tipos de datos

### ğŸ“¦ Carga Optimizada
- Carga en chunks para archivos grandes
- Manejo de errores robusto
- Logging detallado
- VerificaciÃ³n post-carga

## ğŸ“ˆ Archivos Procesados

| Archivo | Tabla | DescripciÃ³n |
|---------|-------|-------------|
| `time_dim.csv` | `time_dim` | Dimensiones de tiempo |
| `fact_table.csv` | `fact_table` | Tabla de hechos principal |
| `Trans_dim.csv` | `trans_dim` | Dimensiones de transacciones |
| `item_dim.csv` | `item_dim` | Dimensiones de productos |
| `store_dim.csv` | `store_dim` | Dimensiones de tiendas |
| `customer_dim.csv` | `customer_dim` | Dimensiones de clientes |

## ğŸ” Monitoreo

### Verificar estado de la BD
```python
from src.utils.db_handler import engine

with engine.connect() as conn:
    result = conn.execute("""
        SELECT table_name, COUNT(*) as rows
        FROM information_schema.tables 
        WHERE table_schema = 'public'
        GROUP BY table_name
    """)
    tables = result.fetchall()
    print(tables)
```

### Logs del Pipeline
- Los logs se guardan en `data_pipeline.log`
- Incluye informaciÃ³n detallada de cada paso
- Errores y advertencias claramente identificados

## ğŸš¨ SoluciÃ³n de Problemas

### Error de conexiÃ³n a Supabase
1. Verificar credenciales en `app/dev.env`
2. Comprobar conectividad de red
3. Verificar que Supabase estÃ© activo

### Error de archivos no encontrados
1. Verificar que los archivos CSV estÃ©n en `app/data/`
2. Comprobar nombres de archivos (case-sensitive)
3. Verificar permisos de lectura

### Error de memoria
- Para archivos muy grandes, el pipeline usa chunks automÃ¡ticamente
- Si persiste, reducir `chunk_size` en `save_db.py`

## ğŸ“ Logs de Ejemplo

```
ğŸš€ Iniciando pipeline de datos...
ğŸ“ Buscando archivos en: /path/to/data
ğŸ“Š Archivos CSV encontrados: 6
   - time_dim.csv
   - fact_table.csv
   - Trans_dim.csv
   - item_dim.csv
   - store_dim.csv
   - customer_dim.csv

ğŸ“ Procesando: /path/to/data/time_dim.csv
âœ… time_dim guardado exitosamente en Supabase
ğŸ“ Procesando: /path/to/data/fact_table.csv
âœ… fact_table guardado exitosamente en Supabase
...

ğŸ‰ Pipeline completado!
âœ… Tablas creadas: 6
âŒ Errores: 0
â±ï¸  Tiempo total: 0:02:15
```

## ğŸ¤ Contribuir

1. Fork el proyecto
2. Crear una rama para tu feature
3. Commit tus cambios
4. Push a la rama
5. Abrir un Pull Request

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. # AiBo
